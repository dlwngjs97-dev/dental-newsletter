import requests
from bs4 import BeautifulSoup
from datetime import datetime
import time
import re
from database import Database

class DentalNewsCrawler:
    def __init__(self):
        self.db = Database()
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }

    def crawl_all(self):
        """ëª¨ë“  ì–¸ë¡ ì‚¬ í¬ë¡¤ë§"""
        print("=== ì¹˜ê³¼ ë‰´ìŠ¤ë ˆí„° í¬ë¡¤ë§ ì‹œì‘ ===")
        print(f"ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

        sources = [
            ('ì¹˜ì˜ì‹ ë³´', self.crawl_generic, 'https://www.dailydental.co.kr', '/news/articleList.html?sc_section_code=S1N1'),
            ('ì¹˜ê³¼ì‹ ë¬¸', self.crawl_generic, 'https://www.dentalnews.or.kr', '/news/articleList.html?sc_section_code=S1N1'),
            ('ë´íƒˆì•„ë¦¬ë‘', self.crawl_generic, 'https://www.dentalarirang.com', '/news/articleList.html?sc_section_code=S1N1')
        ]

        total_new = 0
        for source_name, crawl_func, base_url, path in sources:
            print(f"[{source_name}] í¬ë¡¤ë§ ì¤‘...")
            try:
                new_count = crawl_func(source_name, base_url, path)
                total_new += new_count
                print(f"[{source_name}] {new_count}ê°œ ì‹ ê·œ ê¸°ì‚¬ ìˆ˜ì§‘\n")
            except Exception as e:
                print(f"[{source_name}] ì˜¤ë¥˜ ë°œìƒ: {str(e)}\n")

            time.sleep(2)

        print(f"=== í¬ë¡¤ë§ ì™„ë£Œ: ì´ {total_new}ê°œ ì‹ ê·œ ê¸°ì‚¬ ===")

        exported = self.db.export_to_json()
        print(f"JSON íŒŒì¼ ì—…ë°ì´íŠ¸: {exported}ê°œ ê¸°ì‚¬")

        return total_new

    def crawl_generic(self, source_name, base_url, path):
        """ë²”ìš© í¬ë¡¤ë§ í•¨ìˆ˜"""
        url = base_url + path
        new_articles = 0

        try:
            response = requests.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'lxml')

            # ì—¬ëŸ¬ ì…€ë ‰í„° íŒ¨í„´ ì‹œë„
            selectors = [
                'ul.section-list li',
                '.article-list li',
                '.news-list li',
                'ul li.article',
                'div.list-content ul li',
                '.section-content ul li',
            ]

            articles = []
            for selector in selectors:
                articles = soup.select(selector)
                if len(articles) > 5:  # ìµœì†Œ 5ê°œ ì´ìƒ ë°œê²¬
                    print(f"   âœ… íŒ¨í„´ ë°œê²¬: {selector} ({len(articles)}ê°œ)")
                    break

            if not articles:
                print(f"   âš ï¸ ê¸°ì‚¬ ëª©ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return 0

            for article in articles[:20]:  # ìµœì‹  20ê°œë§Œ
                try:
                    # ì œëª©ê³¼ ë§í¬ ì°¾ê¸° (ì—¬ëŸ¬ íŒ¨í„´ ì‹œë„)
                    title_elem = None
                    link = None

                    title_selectors = [
                        'a.list-titles',
                        '.titles a',
                        'a[href*="article"]',
                        'h2 a',
                        'h3 a',
                        '.title a',
                        'a',
                    ]

                    for ts in title_selectors:
                        title_elem = article.select_one(ts)
                        if title_elem and title_elem.get_text(strip=True):
                            break

                    if not title_elem:
                        continue

                    title = title_elem.get_text(strip=True)
                    link_href = title_elem.get('href', '')

                    if not link_href or 'article' not in link_href.lower():
                        continue

                    article_url = base_url + link_href if link_href.startswith('/') else link_href

                    # ì¸ë„¤ì¼ ì°¾ê¸°
                    thumbnail = ''
                    img_selectors = [
                        'img.list-img',
                        '.thumb img',
                        '.thumbnail img',
                        'img',
                    ]

                    for img_sel in img_selectors:
                        img_tag = article.select_one(img_sel)
                        if img_tag and img_tag.get('src'):
                            src = img_tag['src']
                            if src.startswith('/'):
                                thumbnail = base_url + src
                            elif src.startswith('http'):
                                thumbnail = src
                            break

                    # ë‚ ì§œ ì°¾ê¸°
                    published_date = ''
                    date_selectors = [
                        '.list-dated',
                        '.date',
                        '.byline em',
                        'time',
                        'span.date',
                    ]

                    for date_sel in date_selectors:
                        date_tag = article.select_one(date_sel)
                        if date_tag:
                            published_date = date_tag.get_text(strip=True)
                            break

                    if not published_date:
                        published_date = datetime.now().strftime('%Y-%m-%d')

                    # ê¸°ì‚¬ ìƒì„¸ í˜ì´ì§€ì—ì„œ ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸°
                    content = self._fetch_article_content(article_url)

                    article_data = {
                        'source': source_name,
                        'title': title,
                        'url': article_url,
                        'thumbnail': thumbnail,
                        'content': content,
                        'published_date': published_date,
                        'category': 'ì¹˜ê³¼'
                    }

                    if self.db.insert_article(article_data):
                        new_articles += 1
                        print(f"   ğŸ“° ìˆ˜ì§‘: {title[:40]}...")

                    time.sleep(0.5)

                except Exception as e:
                    print(f"   âš ï¸ ê¸°ì‚¬ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
                    continue

        except Exception as e:
            print(f"   âŒ í¬ë¡¤ë§ ì˜¤ë¥˜: {str(e)}")

        return new_articles

    def _fetch_article_content(self, url):
        """ê¸°ì‚¬ ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸°"""
        try:
            response = requests.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'lxml')

            # ë³¸ë¬¸ ì…€ë ‰í„° (ì—¬ëŸ¬ íŒ¨í„´ ì‹œë„)
            content_selectors = [
                'article .article-view-content-div',
                '.article-content',
                '.news-content',
                '#article-view-content-div',
                '.view-content',
            ]

            for selector in content_selectors:
                content_tag = soup.select_one(selector)
                if content_tag:
                    # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
                    for tag in content_tag.select('script, style, .ad, .related, .share'):
                        tag.decompose()

                    text = content_tag.get_text(strip=True)
                    if len(text) > 100:  # ìµœì†Œ 100ì ì´ìƒ
                        return text[:2000]  # 2000ìê¹Œì§€ë§Œ

        except:
            pass

        return ''


if __name__ == '__main__':
    crawler = DentalNewsCrawler()
    crawler.crawl_all()

#!/usr/bin/env python3
"""
ê¸°ì‚¬ ë§í¬ íŒ¨í„´ ì°¾ê¸° - ê³µê²©ì  ë¶„ì„
"""
import requests
from bs4 import BeautifulSoup
import re

def find_article_links(name, url):
    print(f"\n{'='*80}")
    print(f"{name} - ê¸°ì‚¬ ë§í¬ ì°¾ê¸°")
    print('='*80)

    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, 'lxml')

        # ëª¨ë“  a íƒœê·¸ ì¤‘ ê¸°ì‚¬ ë§í¬ë¡œ ë³´ì´ëŠ” ê²ƒë“¤ ì°¾ê¸°
        all_links = soup.find_all('a', href=True)

        # ê¸°ì‚¬ ë§í¬ íŒ¨í„´ (articleView í¬í•¨)
        article_pattern = re.compile(r'article.*View', re.IGNORECASE)

        article_links = []
        for link in all_links:
            href = link.get('href', '')
            text = link.get_text(strip=True)

            # ê¸°ì‚¬ ë§í¬ë¡œ íŒë‹¨ë˜ëŠ” ì¡°ê±´
            if article_pattern.search(href) and len(text) > 10 and len(text) < 200:
                article_links.append({
                    'href': href,
                    'text': text,
                    'parent_class': link.parent.get('class', []) if link.parent else []
                })

        print(f"\nâœ… ë°œê²¬ëœ ê¸°ì‚¬ ë§í¬: {len(article_links)}ê°œ")

        if article_links:
            print("\n[ì²˜ìŒ 5ê°œ ê¸°ì‚¬]")
            for i, article in enumerate(article_links[:5], 1):
                print(f"\n{i}. ì œëª©: {article['text'][:60]}")
                print(f"   ë§í¬: {article['href'][:80]}")
                print(f"   ë¶€ëª¨ í´ë˜ìŠ¤: {article['parent_class']}")

            # ê³µí†µ íŒ¨í„´ ì°¾ê¸°
            if len(article_links) > 0:
                first_parent = article_links[0]['parent_class']
                print(f"\nğŸ“Œ ì¶”ì²œ ì…€ë ‰í„°:")

                # ë¶€ëª¨ì˜ ë¶€ëª¨ ì°¾ê¸°
                first_link = soup.find('a', href=article_links[0]['href'])
                if first_link and first_link.parent and first_link.parent.parent:
                    container = first_link.parent.parent
                    print(f"   ì»¨í…Œì´ë„ˆ íƒœê·¸: {container.name}")
                    print(f"   ì»¨í…Œì´ë„ˆ í´ë˜ìŠ¤: {container.get('class', [])}")

                    if container.name == 'ul':
                        print(f"\n   âœ… ì‚¬ìš©í•  ì…€ë ‰í„°:")
                        if container.get('class'):
                            class_name = ' '.join(container.get('class'))
                            print(f"      'ul.{class_name.replace(' ', '.')} > li'")
                        else:
                            print(f"      íŠ¹ì • ì»¨í…Œì´ë„ˆ ì•„ë˜ ul > li")

        else:
            print("\nâš ï¸ ê¸°ì‚¬ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

            # ë‹¤ë¥¸ íŒ¨í„´ ì‹œë„
            print("\n[ëŒ€ì•ˆ: 'news' ë˜ëŠ” 'article' í¬í•¨ ë§í¬]")
            alternative_links = [a for a in all_links if ('news' in a.get('href', '').lower() or 'article' in a.get('href', '').lower()) and len(a.get_text(strip=True)) > 10]

            for i, link in enumerate(alternative_links[:5], 1):
                print(f"{i}. {link.get_text(strip=True)[:50]} -> {link.get('href', '')[:60]}")

    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜: {e}")

# ê° ì‚¬ì´íŠ¸ ë¶„ì„
sites = [
    ('ì¹˜ì˜ì‹ ë³´', 'https://www.dailydental.co.kr/news/articleList.html?sc_section_code=S1N1'),
    ('ì¹˜ê³¼ì‹ ë¬¸', 'https://www.dentalnews.or.kr/news/articleList.html?sc_section_code=S1N1'),
    ('ë´íƒˆì•„ë¦¬ë‘', 'https://www.dentalarirang.com/news/articleList.html?sc_section_code=S1N1'),
]

for name, url in sites:
    find_article_links(name, url)

print("\n" + "="*80)
print("ë¶„ì„ ì™„ë£Œ")
print("="*80)
